+++
categories = [""]
tags = [""]
title = "Machine Learning"
date = "2020-09-07T12:59:24-07:00"
draft = false
+++

# books

# kaggle

Learn Intro to Deep Learning Tutorials | Kaggle
https://www.kaggle.com/learn/intro-to-deep-learning

# precision recall

Precision and recall - Wikipedia
https://en.wikipedia.org/wiki/Precision_and_recall


# regularization

batch normalization - Google Search
https://www.google.com/search?q=batch+normalization&sxsrf=ALeKk01t_Nyb0hYW0wty988m_poEp7uMkQ:1601069158148&tbm=isch&source=iu&ictx=1&fir=e-3oYnUZ2pE9BM%252CQR_87h8ilXU2pM%252C%252Fg%252F11f5psmcc3&vet=1&usg=AI4_-kQ_22-uk5wqsPL7vKZgQoCuVNFSnw&sa=X&ved=2ahUKEwiWl7qzn4XsAhXOo54KHRXGDsMQ_B16BAgJEAM#imgrc=e-3oYnUZ2pE9BM

(613) Lecture 7.3 — Regularization | Regularized Linear Regression — [ Machine Learning | Andrew Ng] - YouTube
https://www.youtube.com/watch?v=qbvRdrd0yJ8

# nn

neural networks - What exactly are keys, queries, and values in attention mechanisms? - Cross Validated
https://stats.stackexchange.com/questions/421935/what-exactly-are-keys-queries-and-values-in-attention-mechanisms

# meetup

(Virtual) AutoML Global Summit 2020 | Meetup
https://www.meetup.com/aittg-sfsv/events/272411465/

# preparation

(502) The Bayesian Trap - YouTube
https://www.youtube.com/watch?v=R13BD8qKeTg

Workera | Test
https://workera.ai/candidates/candidate-test

Preparing for the machine learning test
https://workera.ai/resources/preparing-for-the-machine-learning-test/

AI Notes - deeplearning.ai
https://www.deeplearning.ai/ai-notes/

Software Engineering for Data Scientists
https://pythonspeed.com/datascience/

# bootcamp

AICamp
https://learn.xnextcon.com/

Applied Data Science Bootcamp
https://www.mygreatlearning.com/mit-applied-data-science-bootcamp-program-launch?ambassador_code=mgl_live_sep23&utm_source=mit-adsb&utm_campaign=fbgrp&fbclid=IwAR20nl4Hy0QvkUEap7u9tbh1mcirNyj2u1tcWIQdolLz3wq0Ap4UVZaLFwY

Using Data Science to fight Corona and How to Start Learning
https://www.youtube.com/watch?v=w9_hJtYEguU&feature=youtu.be

# stanford
https://developers.google.com/machine-learning/crash-course/embeddings/video-lecture

Index of /notes
http://cs229.stanford.edu/notes/?C=M;O=A
# swe 

Software Engineering for Data Scientists
https://pythonspeed.com/datascience/

# attention networks

[1903.07785] Cloze-driven Pretraining of Self-attention Networks
https://arxiv.org/abs/1903.07785

Linguistic Features · spaCy Usage Documentation
https://spacy.io/usage/linguistic-features#named-entities

[1810.04805] BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding
https://arxiv.org/abs/1810.04805

[1906.08237] XLNet: Generalized Autoregressive Pretraining for Language Understanding
https://arxiv.org/abs/1906.08237

# NLP

14 NLP Research Breakthroughs You Can Apply To Your Business
https://www.topbots.com/most-important-ai-nlp-research/

Beginner's Guide To Hiring AI And Machine Learning Engineers
https://www.topbots.com/guide-hiring-ai-machine-learning-engineers/

Best Research Papers From ICML 2020
https://www.topbots.com/best-research-papers-icml-2020/

Best Research Papers From ACL 2020
https://www.topbots.com/best-research-papers-from-acl-2020/

Reformer, Longformer, and ELECTRA: Key Updates To Transformer Architecture In 2020
https://www.topbots.com/key-updates-to-transformer-architecture-2020/

What Are Major NLP Achievements & Papers From 2019?
https://www.topbots.com/top-ai-nlp-research-papers-2019/

8 Leading Language Models For NLP In 2020
https://www.topbots.com/leading-nlp-language-models-2020/

These 10 AI Companies Are Transforming Marketing In 2020
https://www.topbots.com/ai-companies-transforming-marketing/

Reformer, Longformer, and ELECTRA: Key Updates To Transformer Architecture In 2020
https://www.topbots.com/key-updates-to-transformer-architecture-2020/

The Best NLP Papers From ICLR 2020
https://www.topbots.com/best-nlp-papers-from-iclr-2020/

The Dark Secrets Of BERT
https://www.topbots.com/bert-secrets/

Data Labeling For Natural Language Processing
https://www.topbots.com/data-labeling-for-natural-language-processing/

Beginner's Guide To Hiring AI And Machine Learning Engineers
https://www.topbots.com/guide-hiring-ai-machine-learning-engineers/

(613) BERT Neural Network - EXPLAINED! - YouTube
https://www.youtube.com/watch?v=xI0HHN5XKDo

machine learning - What's the difference between feed-forward and recurrent neural networks? - Cross Validated
https://stats.stackexchange.com/questions/2213/whats-the-difference-between-feed-forward-and-recurrent-neural-networks

(613) Transformer Neural Networks - EXPLAINED! (Attention is all you need) - YouTube
https://www.youtube.com/watch?v=TQQlZhbC5ps

Deconstructing BERT, Part 2: Visualizing the Inner Workings of Attention
https://www.kdnuggets.com/2019/03/deconstructing-bert-part-2-visualizing-inner-workings-attention.html

The Illustrated Transformer – Jay Alammar – Visualizing machine learning one concept at a time.
http://jalammar.github.io/illustrated-transformer/

The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning) – Jay Alammar – Visualizing machine learning one concept at a time.
http://jalammar.github.io/illustrated-bert/

how are the weights for the query key generate from the word embedding - Google Search
https://www.google.com/search?q=how+are+the+weights+for+the+query+key+generate+from+the+word+embedding&oq=how+are+the+weights+for+the+query+key+generate+from+the+word+embedding&aqs=chrome..69i57.19912j0j7&sourceid=chrome&ie=UTF-8

[1706.03762] Attention Is All You Need
https://arxiv.org/abs/1706.03762

Attention and its Different Forms | by Anusha Lihala | Towards Data Science
https://towardsdatascience.com/attention-and-its-different-forms-7fc3674d14dc#:~:text=Multiple%20Queries&text=Multi%2Dhead%20attention%20takes%20this,which%20gives%20us%20h%20heads.

# unet

orobix/retina-unet: Retina blood vessel segmentation with a convolutional neural network
https://github.com/orobix/retina-unet

IEEE Xplore Full-Text PDF:
https://ieeexplore-ieee-org.stanford.idm.oclc.org/stamp/stamp.jsp?tp=&arnumber=7161344

UNet. Introducing Symmetry in Segmentation | by Heet Sankesara | Towards Data Science
https://towardsdatascience.com/u-net-b229b32b4a71

# nlp

Stanford CS 224N | Natural Language Processing with Deep Learning
http://web.stanford.edu/class/cs224n/

Speech and Language Processing
https://web.stanford.edu/~jurafsky/slp3/

gt-nlp-class/eisenstein-nlp-notes.pdf at master · jacobeisenstein/gt-nlp-class
https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf

Neural networks and deep learning
http://neuralnetworksanddeeplearning.com/

Natural Language Processing with PyTorch: Build Intelligent Language Applications Using Deep Learning | Delip Rao, Brian McMahan | download
https://bookos-z1.org/book/3711505/debacd

